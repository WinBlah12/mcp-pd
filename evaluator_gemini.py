# evaluate_tool_selection_gemini.py
import json
import os
import pandas as pd
import google.generativeai as genai
from dotenv import load_dotenv
from tqdm import tqdm
import time
import logging
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- User-Configurable Variables ---
# The total number of tools to present to the LLM in each test.
# This includes 1 correct tool and (NUM_CHOICES - 1) random distractors.
NUM_CHOICES = 2797

# The total number of queries from the input file to test.
# Set to None to test all queries in the input file.
NUM_TESTS = 500

# The LLM to be evaluated for its tool selection capabilities.
MODEL_TO_TEST = "gemini-2.0-flash"

# Number of concurrent threads to run for making API calls.
NUM_WORKERS = 4 

# --- Configuration ---
load_dotenv()

# File Paths
INPUT_CSV = "mcp_zero_openai_queries_v2.csv"  # The dataset generated by the previous script
OUTPUT_CSV = "evaluation_results_gemini_2.0_flash_2797_context.csv" # Changed output file name for Gemini
LOG_FILE = "evaluation_gemini.log"           # Changed log file name

# API Settings
MAX_API_RETRIES = 3

# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(threadName)s - %(levelname)s - %(message)s',
    filename=LOG_FILE,
    filemode='w' # Overwrite log file on each new run
)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.WARNING)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logging.getLogger().addHandler(console_handler)

# --- Thread-Safe Utilities ---
print_lock = threading.Lock()
csv_lock = threading.Lock()
worker_id_map = {}
next_worker_id = 1

def get_worker_id():
    """Assigns a simple, readable ID to each thread for logging."""
    global next_worker_id
    thread_id = threading.get_ident()
    with print_lock:
        if thread_id not in worker_id_map:
            worker_id_map[thread_id] = next_worker_id
            next_worker_id += 1
        return worker_id_map[thread_id]

# --- Gemini Client & Prompt Setup ---
EVALUATION_SYSTEM_PROMPT = """You are a highly intelligent AI agent designed to select the best tool for a user's request.
From the provided list of tools, choose the single most appropriate tool to handle the user's query.
Respond ONLY with a JSON object containing the name of your chosen tool, like this: {"tool_name": "chosen_tool_name"}"""

# Configure the Gemini API client
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Set up generation configuration for consistent, JSON-only output
generation_config = genai.types.GenerationConfig(
    temperature=1.0,
    response_mime_type="application/json" # Enforce JSON output
)

# Instantiate the model with system instructions and generation config
gemini_model = genai.GenerativeModel(
    model_name=MODEL_TO_TEST,
    system_instruction=EVALUATION_SYSTEM_PROMPT,
    generation_config=generation_config
)

def call_evaluation_llm(user_prompt):
    """Calls the specified Gemini model for evaluation with retries."""
    for attempt in range(MAX_API_RETRIES):
        try:
            # The prompt is now a single string, as the system prompt is part of the model config
            response = gemini_model.generate_content(user_prompt)
            return response.text
        except Exception as e:
            # This will catch API errors, content filtering issues, etc.
            logging.error(f"API Error on attempt {attempt + 1}: {e}. Retrying in 5 seconds...")
            time.sleep(5)
    logging.critical(f"API call failed after {MAX_API_RETRIES} retries.")
    return None

def run_single_test(job_data):
    """
    Executes a single evaluation test for a given query and tool set.
    This function is run by each worker thread.
    """
    worker_id = get_worker_id()
    
    # Unpack job data
    test_case = job_data['test_case']
    all_tools_df = job_data['all_tools_df']
    
    correct_tool_name = test_case['tool_name']
    
    # 1. Prepare the tool choices for the LLM
    # Get the correct tool's details
    correct_tool_details = all_tools_df[all_tools_df['tool_name'] == correct_tool_name].iloc[0]
    
    # Select N-1 random distractors, ensuring they are not the correct tool
    distractors_df = all_tools_df[all_tools_df['tool_name'] != correct_tool_name]
    if len(distractors_df) < NUM_CHOICES - 1:
        logging.warning(f"Not enough unique distractors for tool '{correct_tool_name}'. Using all available.")
        distractor_sample = distractors_df
    else:
        distractor_sample = distractors_df.sample(n=NUM_CHOICES - 1, random_state=200)

    # Combine correct tool with distractors and shuffle
    choices = pd.concat([pd.DataFrame([correct_tool_details]), distractor_sample])
    choices = choices.sample(frac=1, random_state=40).reset_index(drop=True) # Shuffle

    # 2. Format the user prompt
    tool_list_str = ""
    for idx, row in choices.iterrows():
        tool_list_str += f"{idx + 1}. name: {row['tool_name']}\n   description: {row['tool_description']}\n\n"

    user_prompt = f"User Query: \"{test_case['generated_query']}\"\n\n" \
                  f"### Available Tools:\n{tool_list_str}" \
                  "Select the single best tool from the list above to address the user query."
    
    # 3. Call the LLM and parse the response
    response_str = call_evaluation_llm(user_prompt)
    llm_choice = "API_FAILURE"
    
    if response_str:
        try:
            # The Gemini model with response_mime_type="application/json" should return valid JSON
            llm_choice = json.loads(response_str).get("tool_name", "MISSING_KEY")
        except json.JSONDecodeError:
            llm_choice = "INVALID_JSON"
            logging.error(f"LLM returned invalid JSON for tool '{correct_tool_name}': {response_str}")

    # 4. Determine correctness and log the result
    is_correct = 1 if llm_choice == correct_tool_name else 0
    log_message = f"Tool: {correct_tool_name}, Persona: {test_case['persona']}, Correct: {bool(is_correct)}, LLM chose: {llm_choice}"
    if is_correct:
        logging.info(f"SUCCESS | {log_message}")
    else:
        logging.warning(f"FAILURE | {log_message}")

    # 5. Prepare the result for saving
    result = {
        'tool_tested': correct_tool_name,
        'persona': test_case['persona'],
        'num_choices': NUM_CHOICES,
        'llm_choice': llm_choice,
        'correct_choice': correct_tool_name,
        'is_correct': is_correct
    }
    
    # Append result to CSV immediately in a thread-safe manner
    with csv_lock:
        pd.DataFrame([result]).to_csv(OUTPUT_CSV, mode='a', header=False, index=False, encoding='utf-8')

def main():
    if not os.getenv("GOOGLE_API_KEY"):
        logging.critical("CRITICAL: GOOGLE_API_KEY not found in .env file. Exiting.")
        return

    # --- 1. Load Data and Prepare Jobs ---
    try:
        test_cases_df = pd.read_csv(INPUT_CSV)
    except FileNotFoundError:
        logging.critical(f"CRITICAL: Input file '{INPUT_CSV}' not found. Please run the generation script first. Exiting.")
        return

    # Create a de-duplicated dataframe of all available tools for the distractor pool
    all_tools_df = test_cases_df[['server_name', 'tool_name', 'tool_description']].drop_duplicates().reset_index(drop=True)

    # --- 2. Handle Resuming from a Previous Run ---
    completed_jobs = set()
    if os.path.exists(OUTPUT_CSV):
        print(f"Resuming from existing results file: '{OUTPUT_CSV}'")
        try:
            df_existing = pd.read_csv(OUTPUT_CSV)
            for _, row in df_existing.iterrows():
                completed_jobs.add((row['tool_tested'], row['persona']))
            print(f"Found {len(completed_jobs)} completed tests to skip.")
        except Exception as e:
            logging.warning(f"Could not parse existing results file: {e}. Starting fresh.")
            # If the file is corrupt, start over by writing the header
            pd.DataFrame(columns=['tool_tested', 'persona', 'num_choices', 'llm_choice', 'correct_choice', 'is_correct']).to_csv(OUTPUT_CSV, index=False)
    else:
        # Create the file with a header if it doesn't exist
        pd.DataFrame(columns=['tool_tested', 'persona', 'num_choices', 'llm_choice', 'correct_choice', 'is_correct']).to_csv(OUTPUT_CSV, index=False)
    
    # Prepare the list of jobs to run
    all_test_cases = [row for _, row in test_cases_df.iterrows()]
    pending_test_cases = [
        tc for tc in all_test_cases if (tc['tool_name'], tc['persona']) not in completed_jobs
    ]

    # Apply the user-defined test limit
    if NUM_TESTS is not None:
        pending_test_cases = pending_test_cases[:NUM_TESTS]
        print(f"!!! TEST MODE: Running a maximum of {len(pending_test_cases)} tests. !!!")

    if not pending_test_cases:
        print("All specified tests are already complete.")
    else:
        print(f"Starting evaluation of {len(pending_test_cases)} queries using {NUM_WORKERS} workers...")
        
        # Package jobs for the executor
        jobs_to_run = [{ 'test_case': tc, 'all_tools_df': all_tools_df } for tc in pending_test_cases]

        # --- 3. Execute Tests with Multithreading ---
        with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
            futures = [executor.submit(run_single_test, job) for job in jobs_to_run]
            for future in tqdm(as_completed(futures), total=len(jobs_to_run), desc="Evaluating Tools"):
                try:
                    future.result()  # Check for exceptions from the worker thread
                except Exception as exc:
                    logging.error(f"A worker thread generated an exception: {exc}")

    # --- 4. Final Analysis and Summary ---
    print("\nEvaluation complete. Calculating final results...")
    try:
        final_df = pd.read_csv(OUTPUT_CSV)
        total_tested = len(final_df)
        if total_tested > 0:
            total_correct = final_df['is_correct'].sum()
            accuracy = (total_correct / total_tested) * 100
            
            print("\n--- OVERALL LLM PERFORMANCE ---")
            print(f"Model Tested:      {MODEL_TO_TEST}")
            print(f"Total Tests Run:   {total_tested}")
            print(f"Total Correct:     {total_correct}")
            print(f"Accuracy:          {accuracy:.2f}%")
            print("-------------------------------")
        else:
            print("No results found to analyze.")
    except Exception as e:
        print(f"Could not read or analyze results file '{OUTPUT_CSV}'. Error: {e}")

if __name__ == "__main__":
    main()